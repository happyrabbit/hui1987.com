---
title: "数据分析一般流程和数据预处理"
author: "林荟 (杜邦先锋商业数据科学家)"
date: "2016-6-2"
output: ioslides_presentation
---

## 介绍

- 幻灯片&R代码：http://scientistcafe.com
- 什么是数据科学家
    - 澄清定义：行业现状，分布，教育技能要求等
    - http://www.xueqing.tv/course/55
- 数据分析的一般流程：问题到数据，数据到信息，信息到价值  

    > Information is not knowledge. Let's not confuse the two. [Edward Deming]
    
- 数据预处理

## 数据分析一般流程

<img src="/Users/happyrabbit/Documents/GitHub/linhui.org/book/Figure/GernalProcess.png" width="800px" />


## 问题到数据

- 理解客户的问题：谁是客户？交流，交流，交流！
- 问题要具体
    - 问题1: 如何提高产品销售量？
    - 问题2: 今年年初推出的新促销手段是不是提高了先锋先玉696玉米种子在西南地区的销售量？
    - 类别：比较、描述、聚类，判别还是回归 
    - 需要什么样的数据：现有数据，数据质量，需要收集的数据，自变量，因变量
- 服装消费者分析：某服装公司要通过精准营销提高效率
    - 哪些品牌？品牌价位？风格质量？
    - 现有消费者数据：人口统计学数据，在线消费，实体店消费
    - 其它关于消费者选择偏好的数据

## 数据到信息

- 流程图：数据准备，数据清理，建模和模型评估
- 数据预处理：清理、变换、缺失值填补等 （非常重要且耗时）
- 建模和评估：数据划分和再抽样，评估标准
    - Occam's razor： William of Pckham (1287-1347)
    - 明确目标
    - 权衡：没有免费的午餐

## 信息到行动

- 行业知识
- 讲故事的能力 

## 数据预处理

在实际分析项目中，根据数据清理的不同阶段，有下面几类数据：

1. 原始数据
1. 技术上正确的数据
1. 可以用于模型的数据
1. 整合后的数据
1. 设置了固定格式的数据

**建议：分别储存每一步得到的数据，以及各个处理过程使用的R代码，使得这个过程尽可能可重复。如果需要检查更改某个环节，也相对容易。**

## 服装消费者数据

1. 人口统计学变量
    - 年龄（age）
    - 性别（gender）
    - 有房还是租房（house）
1. 消费者行为变量
    - 2015年实体店购买该品牌服装花销（store_exp）
    - 2015年在线购买该品牌服装花销（online_exp）
    - 2015年实体店交易次数（store_trans）
    - 2015年在线交易次数（online_trans）
1. 客户认知问卷调查    

## 服装消费者数据

（Q1）：我喜欢买不同品牌的服装，比较它们  
（Q2）：我喜欢买同一个品牌的服装  
（Q3）：品牌的知名度对我来说非常重要  
（Q4）：服装质量对我来说非常重要  
（Q5）：服装风格我喜欢的风格  
（Q6）：我喜欢在实体店购买  
（Q7）：我喜欢在网上购买  
（Q8）：价格对我来说很重要  
（Q9）：我喜欢不同风格的衣服  
（Q10）：我喜欢自己挑选服装，不需要周围人的建议  

- 1（非常不同意）, 2（有点不同意）, 3（中立/不知道）, 4（有点同意）, 5（非常同意）

## 数据清理

- 将错误的年龄观测设置为缺失值
- 将错误的实体店购买观测设置为缺失值
- 通过`summary()`函数检查清理情况

## 缺失值填补

- 填补之前

    1. 了解缺失的原因
    1. 建模的目的：解释和推断 v.s 预测
    
[Ton de Waal, Sander Scholtus, Jeroen Pannekoek. 2011. Handbook of Statistical Data Editing and Imputation. John Wiley & Sons.]
    
- 中位数或众数填补

    1. `imputeMissings`包中的函数`impute()`
    1. `preProcess()`函数
  
## 缺失值填补

- K-近邻填补

    1. “物以类聚”这一思想的统计学表达
    1. K-近邻方法建立在距离的定义之上（通常时欧几里德距离）
    1. `preProcess()`实现
    
    - **注意：算法无法对整行缺失的观测进行填补**

- 袋状树填补
    - Bagging [Bootstrap aggregation的缩写]
    - Bagging, Random Forest and Boosting

## 数据变换

- 中心化和标量化 (这是最基本的)
    
    1. 手动进行
    1. 用`caret`包中的函数preProcess()对多个变量同时进行中心化和标量化
    
- 有偏分布

    $$偏度=\frac{\sum(x_{i}+\bar{x})^{3}}{(n-1)v^{3/2}}$$
    $$v=\frac{\sum(x_{i}=\bar{x})^{2}}{(n-1)}$$

## 数据变换：Box-Cox变换

$$x^{*}=\begin{cases}
\begin{array}{c}
\frac{x^{\lambda}-1}{\lambda}\\
log(x)
\end{array} \begin{array}{c}
if\ \lambda\neq0\\
if\ \lambda=0
\end{array}\end{cases}$$

## 处理离群点

- 可视化
- Z分值

    $$Z_{i}=\frac{Y_{i}-\bar{Y}}{s}$$
    Iglewicz和Hoaglin提出使用修正后的Z分值来判断离群点：
    $$M_{i}=\frac{0.6745(Y_{i}-\bar{Y})}{MAD}$$
    其中MAD是一系列$|Y_{i}-\bar{Y}|$的中位数，称为绝对离差中位数。他们建议将上面修正后的Z分值大于3.5的点标记为可能的离群点。

## 共线性

- `corrplot()`可视化
- 删除高度相关变量：`caret`中的`findCorrelation()`函数

> 算法：处理高度相关变量 （《应用预测模型》3.5小节）
>   
1. 计算自变量的相关系数矩阵
1. 找出相关系数绝对值最大的那对自变量（记为自变量A和B）
1. 计算A和其他自变量相关系数的均值。对B也做同样的计算
1. 如果A的平均相关系数更大，则将A移除；如若不然，移除B
1. 重复步骤2到4，直至所有相关系数的绝对值都低于设定的阈值

## 稀疏变量

通常识别这样的变量有两个法则：

- 不同取值数目和样本量的比值
- 最常见的取值频数和第二常见的取值频数之间的比值
- `caret`包中的`nearZeroVar()`函数

## 编码名义变量

- 什么是名义变量？
- `nnet`包中的`class.ind()`函数
- `caret`包中的`dummyVars()` 函数
- 添加交互效应

## 数据整合和整形

- `apply()`、`lapply()`和`sapply()`
- `plyr`包中的`ddply()`
- `reshape2`中的`melt()`和`dcast()`

## 总结

数据预处理流程：

1. 检查数据：变量分布，是不是存在错误的观测
1. 缺失值填补：了解缺失原因，选择填补方式
1. 数据变换：取决于需要建立的模型，对不符合正态分布假设，变量尺度差异大，有离群值的数据进行变换
1. 检查共线性：找到高度线性相关的变量，决定删除变量，还是使用PCA，CFA这类非监督方法得到不相关的变量线性组合
1. 稀疏变量：查找并且删除稀疏变量
1. 编码名义变量：对于不能作用于分类变量的模型，将分类变量转化成0/1名义变量

## 附录：用到的R包

- caret: 提供获取、使用、评估成百上千个机器学习模型及其拟合效果的系统交互界面，为机器学习提供了结构化的方法并且对一系列机器学习过程进行评估
- e1071: 各类计量经济和机器学习的延伸；我们通过`naiveBayes()`函数进行朴素贝叶斯判别
- gridExtra: 绘图辅助功能，讲不同的图形组合在一起成为图表
- lattice: 建立在核心绘图能力上的格子框架图形
- imputeMissings: 填补缺失值
- RANN: 应用k邻近算法

## 附录：用到的R包

- corrplot: 相关矩阵的高级可视化
- nnet: 拟合单个潜层级的神经网络模型
- car: 回归模型解释和可视化工具，其它附加功能； 其中包括`some()`和`scatterplotMatrix()`函数
- gpairs: 广义散点图；对混合类别和连续变量产生散点图矩阵
- reshape2: 灵活重构和整合数据，主要有两个函数`melt()`和`dcast()`
- psych: 心理计量学方法和抽样调查分析，尤其是因子分析和项目反应模型；
- plyr: 可以将数据分割成更小的数据，然后对分割后的数据进行些操作，最后把操作的结果汇总


